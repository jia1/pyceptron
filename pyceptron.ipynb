{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4248 Assignment 3\n",
    "\n",
    "### Perceptron text classifier in Python with feature selection\n",
    "\n",
    "##### https://github.com/jia1/pyceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tc_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from string import punctuation\n",
    "from porter import PorterStemmer\n",
    "from random import seed, randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = os.path.abspath('tc')\n",
    "stopword_list_file, train_class_list_file, model_file = 'stopword-list', 'train-class-list', 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "max_compromise = 1\n",
    "num_both, num_train = 0, 0\n",
    "train_ratio = 1\n",
    "# train_ratio = 0.8\n",
    "# test_ratio = 1 - train_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds_list = [10] # [5, 10]\n",
    "alpha_list = [0.1] # [0.02, 0.03, 0.05, 0.07, 0.1]\n",
    "max_iterations_list = [500] # [500, 1000, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 0\n",
    "class_list = []\n",
    "class_to_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_prune_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_count = {}\n",
    "nxx_list = ['n10', 'n11']\n",
    "nxx_map = {\n",
    "    'n00': 'n10',\n",
    "    'n01': 'n11'\n",
    "}\n",
    "nxx_to_word_to_class_to_chi = { n: {} for n in nxx_list }\n",
    "class_to_word_to_chi = {}\n",
    "class_to_feat_chi_tup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_word_list = {}\n",
    "class_to_vocab_to_tfidf = {}\n",
    "class_to_feat_tfidf_tup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_feat_set = {}\n",
    "class_to_feat_list_sort_by_lex = {}\n",
    "class_to_feat_to_index = {}\n",
    "class_to_feat_mat = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_weights = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PorterStemmer()\n",
    "seed(4248)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_and_filter_line(ln):\n",
    "    if all(x in ln for x in [':', '@']):\n",
    "        return []\n",
    "    tokens = map(lambda t: t.strip().strip(punctuation).lower(), ln.split(' '))\n",
    "    return list(filter(lambda t: t and len(t) > 2 and t.isalpha() and t not in stop_list, tokens))\n",
    "\n",
    "def get_word_to_count(word_list):\n",
    "    word_to_count = {}\n",
    "    num_words = len(word_list)\n",
    "    prev_unigram = word_list[0]\n",
    "    for i in range(1, num_words):\n",
    "        curr_unigram = word_list[i]\n",
    "        ngrams = [curr_unigram, '{} {}'.format(prev_unigram, curr_unigram)]\n",
    "        for ngram in ngrams:\n",
    "            if ngram not in word_to_count:\n",
    "                word_to_count[ngram] = 1\n",
    "            else:\n",
    "                word_to_count[ngram] += 1\n",
    "        prev_unigram = curr_unigram\n",
    "    return word_to_count\n",
    "\n",
    "def get_weaker_word_to_count(word_to_count):\n",
    "    fin_word_to_count = {}\n",
    "    for compromise in range(1, max_compromise + 1):\n",
    "        if fin_word_to_count:\n",
    "            break\n",
    "        fin_word_to_count = { word: count for word, count in word_to_count.items() \\\n",
    "                             if count >= k - compromise }\n",
    "        for len_gram in range(2, 0, -1):\n",
    "            fin_word_to_count = { word: count for word, count in fin_word_to_count.items() \\\n",
    "                                 if len(word.split(' ')) >= len_gram }\n",
    "            if fin_word_to_count:\n",
    "                break\n",
    "    return fin_word_to_count\n",
    "\n",
    "def is_in(a, b):\n",
    "    return 1 if a in b else 0\n",
    "\n",
    "def count_nxx(nxx, w, c):\n",
    "    nxx_value = 0\n",
    "    if nxx == 'n10':\n",
    "        for class_name in filter(lambda x: x != c, class_list):\n",
    "            for text in class_to_text[class_name]:\n",
    "                nxx_value += is_in(w, text_to_count[text])\n",
    "    elif nxx == 'n11':\n",
    "        for text in class_to_text[c]:\n",
    "            nxx_value += is_in(w, text_to_count[text])\n",
    "    return nxx_value\n",
    "\n",
    "def calc_chi_square(w, c):\n",
    "    nxx_to_count = {}\n",
    "    for n in nxx_list:\n",
    "        if w not in nxx_to_word_to_class_to_chi[n]:\n",
    "            nxx_to_word_to_class_to_chi[n][w] = {}\n",
    "        if c not in nxx_to_word_to_class_to_chi[n][w]:\n",
    "            nxx_to_word_to_class_to_chi[n][w][c] = count_nxx(n, w, c)\n",
    "        nxx_to_count[n] = nxx_to_word_to_class_to_chi[n][w][c]\n",
    "    for n, nn in nxx_map.items():\n",
    "        nxx_to_count[n] = num_train - nxx_to_word_to_class_to_chi[nn][w][c]\n",
    "    n00, n01, n10, n11 = nxx_to_count['n00'], nxx_to_count['n01'], nxx_to_count['n10'], nxx_to_count['n11']\n",
    "    return ((n11+n10+n01+n00)*(n11*n00-n10*n01)**2)/((n11+n01)*(n11+n10)*(n10+n00)*(n01+n00))\n",
    "\n",
    "def put_chi(c, w, chi_value):\n",
    "    global class_to_word_to_chi\n",
    "    if w not in class_to_word_to_chi[c]:\n",
    "        class_to_word_to_chi[c][w] = chi_value\n",
    "    else:\n",
    "        class_to_word_to_chi[c][w] = max(class_to_word_to_chi[c][w], chi_value)\n",
    "\n",
    "def gen_feat_by_chi():\n",
    "    global class_to_feat_chi_tup\n",
    "    max_feat_vec_len = sys.maxsize\n",
    "    class_to_feat_sorted = { c: [] for c in class_list }\n",
    "    for c in class_to_word_to_chi:\n",
    "        class_to_feat_sorted[c] = sorted(class_to_word_to_chi[c].items(), key = lambda x: x[1], reverse = True)\n",
    "        max_feat_vec_len = min(max_feat_vec_len, len(class_to_feat_sorted[c]))\n",
    "    max_feat_vec_len *= feat_prune_ratio \n",
    "    class_to_feat_chi_tup = { c: class_to_feat_sorted[c][:int(max_feat_vec_len)] for c in class_to_feat_sorted }\n",
    "\n",
    "def gen_feat_by_tfidf():\n",
    "    global class_to_vocab_to_tfidf\n",
    "    \n",
    "    for c in class_list:\n",
    "        for text in class_to_text[c]:\n",
    "            word_list = text_to_word_list[text]\n",
    "            prev_unigram = word_list[0]\n",
    "            class_to_vocab_to_tfidf[c][prev_unigram] = 0\n",
    "            for i in range(1, len(word_list)):\n",
    "                curr_unigram = word_list[i]\n",
    "                bigram = '{} {}'.format(prev_unigram, curr_unigram)\n",
    "                class_to_vocab_to_tfidf[c][curr_unigram] = 0\n",
    "                class_to_vocab_to_tfidf[c][bigram] = 0\n",
    "                prev_unigram = curr_unigram\n",
    "    for c in class_list:\n",
    "        for text in class_to_text[c]:\n",
    "            word_list = text_to_word_list[text]\n",
    "            prev_unigram = word_list[0]\n",
    "            class_to_vocab_to_tfidf[c][prev_unigram] = 0\n",
    "            for i in range(1, len(word_list)):\n",
    "                curr_unigram = word_list[i]\n",
    "                bigram = '{} {}'.format(prev_unigram, curr_unigram)\n",
    "                class_to_vocab_to_tfidf[c][curr_unigram] += 1\n",
    "                class_to_vocab_to_tfidf[c][bigram] += 1\n",
    "                prev_unigram = curr_unigram\n",
    "\n",
    "    for c in class_list:\n",
    "        num_texts = len(class_to_text[c])\n",
    "        for v in class_to_vocab_to_tfidf[c]:\n",
    "            class_to_vocab_to_tfidf[c][v] = math.log(num_texts / (1 + class_to_vocab_to_tfidf[c][v]))\n",
    "                \n",
    "    max_feat_vec_len = sys.maxsize\n",
    "    class_to_feat_sorted = { c: [] for c in class_list }\n",
    "    for c in class_to_word_to_chi:\n",
    "        class_to_feat_sorted[c] = sorted(class_to_vocab_to_tfidf[c].items(), key = lambda x: x[1], reverse = True)\n",
    "        max_feat_vec_len = min(max_feat_vec_len, len(class_to_feat_sorted[c]))\n",
    "    max_feat_vec_len *= feat_prune_ratio \n",
    "    class_to_vocab_to_tfidf = { c: class_to_feat_sorted[c][:int(max_feat_vec_len)] for c in class_to_feat_sorted }\n",
    "\n",
    "def feat_select():\n",
    "    # gen_feat_by_tfidf()\n",
    "    for c in class_list:\n",
    "        for text in class_to_text[c]:\n",
    "            for w in text_to_count[text]:\n",
    "                put_chi(c, w, calc_chi_square(w, c))\n",
    "    gen_feat_by_chi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
    "# Split data_mat into num_folds number of folds\n",
    "def get_folds(data_mat, num_folds):\n",
    "    folds = []\n",
    "    data_clone = list(data_mat)\n",
    "    fold_size = int(len(data_mat) / num_folds)\n",
    "    for i in range(num_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(data_clone))\n",
    "            fold.append(data_clone.pop(index))\n",
    "        folds.append(fold)\n",
    "    return folds\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def get_accuracy(predicted, actual):\n",
    "    num_correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if predicted[i] == actual[i]:\n",
    "            num_correct += 1\n",
    "    return num_correct / len(actual) * 100\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def get_cross_validation_scores(data_mat, algorithm, num_folds, *args):\n",
    "    folds = get_folds(data_mat, num_folds)\n",
    "    scores = []\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = [row for fold in train_set for row in fold]\n",
    "        test_set = []\n",
    "        for row in fold:\n",
    "            row_clone = list(row)\n",
    "            test_set.append(row_clone)\n",
    "            row_clone[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = get_accuracy(predicted, actual)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row) - 1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1 if activation >= 0 else 0\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, alpha, max_iterations = 1000):\n",
    "    weights = [0 for i in range(len(train[0]))]\n",
    "    for _ in range(max_iterations):\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            if error:\n",
    "                weights[0] = weights[0] + alpha * error\n",
    "                for i in range(len(row) - 1):\n",
    "                    weights[i + 1] += alpha * error * row[i]\n",
    "    return weights\n",
    "\n",
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, alpha, max_iterations):\n",
    "    predictions = list()\n",
    "    weights = train_weights(train, alpha, max_iterations)\n",
    "    for row in test:\n",
    "        prediction = predict(row, weights)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopword_list_file, 'r') as s:\n",
    "    stop_list = list(map(lambda ln: ln.strip(), s.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_dir, sub_dir, files in os.walk(src_dir):\n",
    "    if not files:\n",
    "        class_list = sub_dir\n",
    "        num_class = len(class_list)\n",
    "        class_to_text = { c: set() for c in class_list }\n",
    "        class_to_word_to_chi = { c: {} for c in class_list }\n",
    "        class_to_feat_chi_tup = { c: set() for c in class_list }\n",
    "        class_to_vocab_to_tfidf = { c: {} for c in class_list }\n",
    "        class_to_feat_tfidf_tup = { c: set() for c in class_list }\n",
    "        class_to_feat_set = { c: set() for c in class_list }\n",
    "        class_to_feat_list_sort_by_lex = { c: [] for c in class_list }\n",
    "        class_to_feat_to_index = { c: {} for c in class_list }\n",
    "        continue\n",
    "\n",
    "    curr_class = re.split('[(\\\\\\\\)(\\\\)(\\/)]', curr_dir)[-1] # curr_dir.split('\\\\')[-1]\n",
    "    curr_num_files = len(files)\n",
    "    num_both += curr_num_files\n",
    "    curr_num_train = int(curr_num_files * train_ratio)\n",
    "    num_train += curr_num_train\n",
    "    for i in range(curr_num_train):\n",
    "        file = files[i]\n",
    "        flat_text = []\n",
    "        with open(os.path.join(curr_dir, file), 'r') as f:\n",
    "            for line in map(lambda ln: strip_and_filter_line(ln), f.readlines()):\n",
    "                flat_text.extend(list(map(lambda word: p.stem(word, 0, len(word) - 1), line)))\n",
    "            word_to_count = get_word_to_count(flat_text)\n",
    "            fin_word_to_count = { word: count for word, count in word_to_count.items() if count >= k }\n",
    "            if not fin_word_to_count:\n",
    "                fin_word_to_count = get_weaker_word_to_count(word_to_count)\n",
    "            sum_count = sum(fin_word_to_count.values())\n",
    "            class_to_text[curr_class].add(file)\n",
    "            text_to_count[file] = { word: count / sum_count for word, count in fin_word_to_count.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_word_to_chi = { c: {} for c in class_list }\n",
    "class_to_feat_chi_tup = { c: set() for c in class_list }\n",
    "class_to_word_to_num_text = { c: {} for c in class_list }\n",
    "class_to_feat_tfidf_tup = { c: set() for c in class_list }\n",
    "feat_select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_feat_set = { c: set() for c in class_list }\n",
    "\n",
    "for c in class_to_feat_chi_tup:\n",
    "    for p in class_to_feat_chi_tup[c]:\n",
    "        w = p[0]\n",
    "        class_to_feat_set[c].add(w)\n",
    "    curr_num_feat = len(class_to_feat_set[c])\n",
    "    num_feat_per_neg_class = curr_num_feat // (num_class - 1)\n",
    "    for nc in class_to_feat_chi_tup:\n",
    "        if nc != c:\n",
    "            num_added = 0\n",
    "            for t in class_to_feat_chi_tup:\n",
    "                class_to_feat_set[c].add(t[0])\n",
    "                if num_added >= num_feat_per_neg_class:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_feat_list_sort_by_lex = { c: sorted(list(class_to_feat_set[c])) for c in class_list }\n",
    "class_to_feat_to_index = { c: {} for c in class_list }\n",
    "\n",
    "for c in class_to_feat_list_sort_by_lex:\n",
    "    for i in range(len(class_to_feat_list_sort_by_lex[c])):\n",
    "        class_to_feat_to_index[c][class_to_feat_list_sort_by_lex[c][i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "class_to_feat_mat = { c: [] for c in class_list }\n",
    "for c in class_list:\n",
    "    for d in class_list:\n",
    "        texts = class_to_text[d]\n",
    "        num_texts = len(texts)\n",
    "        texts = iter(texts)\n",
    "        if c != d:\n",
    "            num_texts_to_train = int(num_texts * train_ratio / (num_class - 1))\n",
    "        else:\n",
    "            num_texts_to_train = num_texts\n",
    "        for i in range(num_texts_to_train):\n",
    "            text = next(texts)\n",
    "            feat_vec = [0 for i in range(len(class_to_feat_to_index[d]) + 1)]\n",
    "            for word in text_to_count[text]:\n",
    "                if word in class_to_feat_to_index[d]:\n",
    "                    index = class_to_feat_to_index[d][word]\n",
    "                    feat_vec[index] = text_to_count[text][word]\n",
    "            feat_vec[-1] = 1 if c == d else 0\n",
    "            class_to_feat_mat[c].append(feat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = class_to_feat_mat\n",
    "\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    for alpha in alpha_list:\n",
    "        for max_iterations in max_iterations_list:\n",
    "            print('{}-fold cross validation'.format(num_folds))\n",
    "            print('Learning rate: {}, maximum number of iterations: {}'.format(alpha, max_iterations))\n",
    "            for c in class_list:\n",
    "                scores = get_cross_validation_scores(data[c], perceptron, num_folds, alpha, max_iterations)\n",
    "                print('Class: {}'.format(c))\n",
    "                print('Cross validation scores: {}'.format(scores))\n",
    "                print('Mean accuracy: {:.2f}%'.format(sum(scores) / num_folds))\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in class_list:\n",
    "    class_to_weights[c] = train_weights(data[c], alpha_list[0], max_iterations_list[0])\n",
    "\n",
    "with open(model_file, 'w') as m:\n",
    "    lines_to_write = []\n",
    "    lines_to_write.append(str(class_list))\n",
    "    lines_to_write.append(str(class_to_feat_to_index))\n",
    "    lines_to_write.append(str(class_to_weights))\n",
    "    m.write('\\n'.join(lines_to_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from string import punctuation\n",
    "from porter import PorterStemmer\n",
    "\n",
    "'''\n",
    "args = sys.argv\n",
    "if len(args) != 5:\n",
    "    sys.exit('Usage: python3 tc_test.py stopword-list model test-list test-class-list')\n",
    "'''\n",
    "\n",
    "k = 3\n",
    "max_compromise = 1\n",
    "text_to_class = {}\n",
    "lines_to_write = []\n",
    "score, total = 0, 0\n",
    "\n",
    "# stopword_list_file, model_file, test_list_file, test_class_list_file = args[1:]\n",
    "stopword_list_file, model_file, test_list_file, test_class_list_file = 'stopword-list', 'model', 'test-list-local', 'test-class-list-local'\n",
    "p = PorterStemmer()\n",
    "\n",
    "def strip_and_filter_line(ln):\n",
    "    if all(x in ln for x in [':', '@']):\n",
    "        return []\n",
    "    tokens = map(lambda t: t.strip().strip(punctuation).lower(), ln.split(' '))\n",
    "    return list(filter(lambda t: t and len(t) > 2 and t.isalpha() and t not in stop_list, tokens))\n",
    "\n",
    "def get_word_to_count(word_list):\n",
    "    word_to_count = {}\n",
    "    num_words = len(word_list)\n",
    "    prev_unigram = word_list[0]\n",
    "    for i in range(1, num_words):\n",
    "        curr_unigram = word_list[i]\n",
    "        ngrams = [curr_unigram, '{} {}'.format(prev_unigram, curr_unigram)]\n",
    "        for ngram in ngrams:\n",
    "            if ngram not in word_to_count:\n",
    "                word_to_count[ngram] = 1\n",
    "            else:\n",
    "                word_to_count[ngram] += 1\n",
    "        prev_unigram = curr_unigram\n",
    "    return word_to_count\n",
    "\n",
    "def get_weaker_word_to_count(word_to_count):\n",
    "    fin_word_to_count = {}\n",
    "    for compromise in range(1, max_compromise + 1):\n",
    "        if fin_word_to_count:\n",
    "            break\n",
    "        fin_word_to_count = { word: count for word, count in word_to_count.items() \\\n",
    "                             if count >= k - compromise }\n",
    "        for len_gram in range(2, 0, -1):\n",
    "            fin_word_to_count = { word: count for word, count in fin_word_to_count.items() \\\n",
    "                                 if len(word.split(' ')) >= len_gram }\n",
    "            if fin_word_to_count:\n",
    "                break\n",
    "    return fin_word_to_count\n",
    "\n",
    "def get_activation(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row) - 1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return activation\n",
    "\n",
    "def predict(activation):\n",
    "    return 1 if activation >= 0 else 0\n",
    "\n",
    "'''\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row) - 1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1 if activation >= 0 else 0\n",
    "'''\n",
    "\n",
    "with open(stopword_list_file, 'r') as s:\n",
    "    stop_list = list(map(lambda ln: ln.strip(), s.readlines()))\n",
    "\n",
    "with open(model_file, 'r') as m:\n",
    "    lines = list(map(lambda w: ast.literal_eval(w), m.readlines()))\n",
    "    class_list, class_to_feat_to_index, class_to_weights = lines\n",
    "\n",
    "with open(test_class_list_file, 'r') as a:\n",
    "    lines = map(lambda ln: ln.strip().split(' '), a.readlines())\n",
    "    for ln in lines:\n",
    "        file, curr_class = ln\n",
    "        # text = file.split('/')[-1]\n",
    "        text = re.split('[(\\\\\\\\)(\\\\)(\\/)]', file)[-1]\n",
    "        text_to_class[text] = curr_class\n",
    "\n",
    "with open(test_list_file, 'r') as t:\n",
    "    # lines = map(lambda ln: ln.strip(), t.readlines())\n",
    "    lines = map(lambda ln: ln.strip().split(' ')[0], t.readlines())\n",
    "    for ln in lines:\n",
    "        file = ln\n",
    "        # text = file.split('/')[-1]\n",
    "        text = re.split('[(\\\\\\\\)(\\\\)(\\/)]', file)[-1]\n",
    "        flat_text = []\n",
    "        with open(file, 'r') as f:\n",
    "            for line in map(lambda ln: strip_and_filter_line(ln), f.readlines()):\n",
    "                flat_text.extend(list(map(lambda word: p.stem(word, 0, len(word) - 1), line)))\n",
    "            word_to_count = get_word_to_count(flat_text)\n",
    "            fin_word_to_count = { word: count for word, count in word_to_count.items() if count >= k }\n",
    "            if not fin_word_to_count:\n",
    "                fin_word_to_count = get_weaker_word_to_count(word_to_count)\n",
    "            sum_count = sum(fin_word_to_count.values())\n",
    "            normalized_word_to_count = { word: count / sum_count for word, count in fin_word_to_count.items() }\n",
    "            instance_class_to_output = { c: 0 for c in class_list }\n",
    "            for c in class_list:\n",
    "                feat_vec = [0 for i in range(len(class_to_feat_to_index[c]))]\n",
    "                for w in class_to_feat_to_index[c]:\n",
    "                    if w in normalized_word_to_count:\n",
    "                        index = class_to_feat_to_index[c][w]\n",
    "                        feat_vec[index] = normalized_word_to_count[w]\n",
    "                instance_class_to_output[c] = get_activation(feat_vec, class_to_weights[c])\n",
    "            instance_class_to_output = sorted(instance_class_to_output.items(), key = lambda x: x[1], reverse = True)\n",
    "            instance_class_to_output = list(filter(lambda x: x[1] != 0, instance_class_to_output))\n",
    "            lines_to_write.append('{} {}\\n'.format(file, instance_class_to_output))\n",
    "            total += 1\n",
    "            if text_to_class[text] == instance_class_to_output[0][0]:\n",
    "                score += 1\n",
    "\n",
    "print(score)\n",
    "print(total)\n",
    "\n",
    "with open('answer', 'w') as f:\n",
    "    f.writelines(lines_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model-k33-f3\n",
    "530\n",
    "\n",
    "model-k33-f5\n",
    "756\n",
    "\n",
    "model-k33-f7\n",
    "724\n",
    "\n",
    "model-k43-f7\n",
    "720\n",
    "\n",
    "model-k53-f7\n",
    "650\n",
    "\n",
    "model-k54-f7\n",
    "500\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
