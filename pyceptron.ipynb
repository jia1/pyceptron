{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4248 Assignment 3\n",
    "\n",
    "### Perceptron text classifier in Python with feature selection\n",
    "\n",
    "##### https://github.com/jia1/pyceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from decimal import Decimal\n",
    "from string import punctuation\n",
    "from porter import PorterStemmer\n",
    "\n",
    "p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = os.path.abspath('tc')\n",
    "\n",
    "stopword_list_file = 'stopword-list'\n",
    "train_class_list_file = 'train-class-list'\n",
    "test_list_file, test_class_list_file = 'test-list', 'test-class-list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class_to_text\n",
    "<class 'dict'>\n",
    "KEY: <class 'str'>\n",
    "VAL: <class 'set'>\n",
    "E.g: {'c1': {'0000', '0001', '0002', ... }, 'c2': {'1000', '1001', '1002', ... } }\n",
    "\n",
    "text_to_freq\n",
    "<class 'dict'>\n",
    "KEY: <class 'str'>\n",
    "VAL: <class 'dict'>\n",
    "     KEY: <class 'str'>\n",
    "     VAL: <class 'int'>\n",
    "E.g: {'0000': {'word0': 1, 'word1': 3, 'word2': 5, ... }, '0001': { ... }}\n",
    "\n",
    "class_to_feat_chi_tup\n",
    "<class 'dict'>\n",
    "KEY: <class 'str'>\n",
    "VAL: <class 'list'>\n",
    "     VAL: <class 'tuple'>\n",
    "          <class 'str'>\n",
    "          <class 'float'>\n",
    "E.g: {'c1': [ ('word1', 99.00), ('word2', 90.00), ... ], 'c2': [ ... ] }\n",
    "\n",
    "nxx_dict\n",
    "chi_dict\n",
    "'''\n",
    "\n",
    "k = 4\n",
    "num_both, num_train = 0, 0\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "feat_prune_ratio = 0.4\n",
    "\n",
    "class_list = []\n",
    "class_to_text, text_to_freq = {}, {}\n",
    "class_to_feat_chi_tup = {}\n",
    "class_to_feat_set = {}\n",
    "class_to_feat_list_sort_by_chi = {}\n",
    "\n",
    "nxxs = ['n10', 'n11']\n",
    "nxxs_map = {\n",
    "    'n10': 'n00',\n",
    "    'n11': 'n01'\n",
    "}\n",
    "nxx_dict = { n: {} for n in nxxs }\n",
    "chi_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Split each line by space into tokens\n",
    "# B. Strip all default white space characters from each token\n",
    "# C. Remove punctuation from each token\n",
    "# D. Return a list of tokens which are not stop words\n",
    "\n",
    "def strip_and_filter_line(ln):\n",
    "    tokens = map(lambda t: t.strip().strip(punctuation).lower(), ln.split(' '))\n",
    "    return list(filter(lambda t: t and len(t) > 2 and t.isalpha() and t not in stop_list, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in(a, b):\n",
    "    return 1 if a in b else 0\n",
    "\n",
    "def count_nxx(nxx, w, c):\n",
    "    global class_list, class_to_text, text_to_freq\n",
    "    answer = 0\n",
    "    if nxx == 'n10':\n",
    "        for class_name in filter(lambda x: x != c, class_list):\n",
    "            for text in class_to_text[class_name]:\n",
    "                answer += is_in(w, text_to_freq[text])\n",
    "    elif nxx == 'n11':\n",
    "        for text in class_to_text[c]:\n",
    "            answer += is_in(w, text_to_freq[text])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n00 is the number of training texts that do not contain w and are not in class c.\n",
    "# n01 is the number of training texts that do not contain w and are in class c.\n",
    "# n10 is the number of training texts that contain w and are not in class c.\n",
    "# n11 is the number of training texts that contain w and are in class c.\n",
    "\n",
    "# n00 = num_train - n10\n",
    "# n01 = num_train - n11\n",
    "# n10 = num_train - n00\n",
    "# n11 = num_train - n01\n",
    "\n",
    "def chi_square(w, c):\n",
    "    global num_train, nxxs, nxxs_map, nxx_dict\n",
    "    ns_dict = {}\n",
    "    for n in nxxs:\n",
    "        if w not in nxx_dict[n]:\n",
    "            nxx_dict[n][w] = {}\n",
    "        if c not in nxx_dict[n][w]:\n",
    "            nxx_dict[n][w][c] = count_nxx(n, w, c)\n",
    "        ns_dict[n] = nxx_dict[n][w][c]\n",
    "    for n, nn in nxxs_map.items():\n",
    "        ns_dict[n] = num_train - nxx_dict[nn][w][c]\n",
    "    n00, n01, n10, n11 = ns_dict['n00'], ns_dict['n01'], ns_dict['n10'], ns_dict['n11']\n",
    "    return ((n11+n10+n01+n00)*(n11*n00-n10*n01)**2)/((n11+n01)*(n11+n10)*(n10+n00)*(n01+n00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_chi_dict(c, w, chi_square_value):\n",
    "    global chi_dict\n",
    "    if w not in chi_dict[c]:\n",
    "        chi_dict[c][w] = chi_square_value\n",
    "    else:\n",
    "        chi_dict[c][w] = max(chi_dict[c][w], chi_square_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feat():\n",
    "    global class_list, chi_dict, class_to_feat_chi_tup\n",
    "    max_feat_len = sys.maxsize\n",
    "    feat_queue_dict = { c: [] for c in class_list }\n",
    "    for c in chi_dict:\n",
    "        feat_queue_dict[c] = sorted(chi_dict[c].items(), key = lambda x: x[1], reverse = True)\n",
    "        max_feat_len = min(max_feat_len, len(feat_queue_dict[c]))\n",
    "    max_feat_len *= feat_prune_ratio \n",
    "    class_to_feat_chi_tup = { c: feat_queue_dict[c][:int(max_feat_len)] for c in feat_queue_dict }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_select():\n",
    "    global class_list, class_to_text, text_to_freq, class_to_feat_chi_tup\n",
    "    for c in class_list:\n",
    "        for text in class_to_text[c]:\n",
    "            for w in text_to_freq[text]:\n",
    "                put_chi_dict(c, w, chi_square(w, c))\n",
    "                gen_feat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopword_list_file, 'r') as s:\n",
    "    stop_list = list(map(lambda ln: ln.strip(), s.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_class_list_file, 'r') as t:\n",
    "    lines = map(lambda ln: ln.strip().split(' '), t.readlines())\n",
    "    for ln in lines:\n",
    "        file, curr_class = ln\n",
    "        text = file.split('/')[-1]\n",
    "        num_both += 1\n",
    "        num_train += 1\n",
    "        flat_text, freq_dict = [], {}\n",
    "        with open(file, 'r') as f:\n",
    "            processed_lines = map(lambda ln: strip_and_filter_line(ln), f.readlines())\n",
    "            for line in processed_lines:\n",
    "                flat_text.extend(list(map(lambda word: p.stem(word, 0, len(word) - 1), line)))\n",
    "            num_words = len(flat_text)\n",
    "            last_index = num_words - 1\n",
    "            for i in range(num_words):\n",
    "                word = flat_text[i]\n",
    "                if word not in freq_dict:\n",
    "                    freq_dict[word] = 1\n",
    "                else:\n",
    "                    freq_dict[word] += 1\n",
    "                if i < last_index:\n",
    "                    bigram = '{} {}'.format(word, flat_text[i + 1])\n",
    "                    if bigram not in freq_dict:\n",
    "                        freq_dict[bigram] = 1\n",
    "                    else:\n",
    "                        freq_dict[bigram] += 1\n",
    "            fin_freq_dict = { word: freq for word, freq in freq_dict.items() if freq >= k }\n",
    "            compromise = 1\n",
    "            while not fin_freq_dict:\n",
    "                fin_freq_dict = { word: freq for word, freq in freq_dict.items() if freq >= k - compromise }\n",
    "                compromise += 1\n",
    "            sum_freq = sum(fin_freq_dict.values())\n",
    "            if curr_class not in class_to_text:\n",
    "                class_to_text[curr_class] = set()\n",
    "            else:\n",
    "                class_to_text[curr_class].add(text)\n",
    "            if text not in text_to_freq:\n",
    "                text_to_freq[text] = {}\n",
    "            else:\n",
    "                text_to_freq[text] = { word: freq / sum_freq for word, freq in fin_freq_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_dir, sub_dir, files in os.walk(src_dir):\n",
    "    if not files:\n",
    "        class_list = sub_dir\n",
    "        num_class = len(class_list)\n",
    "        class_to_text = { c: set() for c in class_list }\n",
    "        chi_dict = { c: {} for c in class_list }\n",
    "        class_to_feat_chi_tup = { c: set() for c in class_list }\n",
    "        class_to_feat_set = { c: set() for c in class_list }\n",
    "        class_to_feat_to_index = { c: {} for c in class_list }\n",
    "        continue\n",
    "    curr_class = re.split('[(\\\\\\\\)(\\\\)(\\/)]', curr_dir)[-1] # curr_dir.split('\\\\')[-1]\n",
    "    curr_num_files = len(files)\n",
    "    num_both += curr_num_files\n",
    "    curr_num_train = int(curr_num_files * train_ratio)\n",
    "    num_train += curr_num_train\n",
    "    for i in range(curr_num_train):\n",
    "        file = files[i]\n",
    "        flat_text, freq_dict = [], {}\n",
    "        with open(os.path.join(curr_dir, file), 'r') as f:\n",
    "            processed_lines = map(lambda ln: strip_and_filter_line(ln), f.readlines())\n",
    "            for line in processed_lines:\n",
    "                flat_text.extend(list(map(lambda word: p.stem(word, 0, len(word) - 1), line)))\n",
    "            num_words = len(flat_text)\n",
    "            last_index = num_words - 1\n",
    "            for i in range(num_words):\n",
    "                word = flat_text[i]\n",
    "                if word not in freq_dict:\n",
    "                    freq_dict[word] = 1\n",
    "                else:\n",
    "                    freq_dict[word] += 1\n",
    "                if i < last_index:\n",
    "                    bigram = '{} {}'.format(word, flat_text[i + 1])\n",
    "                    if bigram not in freq_dict:\n",
    "                        freq_dict[bigram] = 1\n",
    "                    else:\n",
    "                        freq_dict[bigram] += 1\n",
    "            fin_freq_dict = { word: freq for word, freq in freq_dict.items() if freq >= k }\n",
    "            compromise = 1\n",
    "            while not fin_freq_dict:\n",
    "                fin_freq_dict = { word: freq for word, freq in freq_dict.items() if freq >= k - compromise }\n",
    "                compromise += 1\n",
    "            sum_freq = sum(fin_freq_dict.values())\n",
    "            class_to_text[curr_class].add(file)\n",
    "            text_to_freq[file] = { word: freq / sum_freq for word, freq in fin_freq_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "pp.pprint(text_to_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "\n",
    "class_to_feat_chi_tup = { c: set() for c in class_list }\n",
    "feat_select()\n",
    "\n",
    "stop = time()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "pp.pprint(class_to_feat_chi_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_feat_set = { c: {} for c in class_list }\n",
    "for c in class_to_feat_chi_tup:\n",
    "    for p in class_to_feat_chi_tup[c]:\n",
    "        w = p[0]\n",
    "        class_to_feat_set[c].add(w)\n",
    "    for nc in class_to_feat_chi_tup:\n",
    "        if nc != c:\n",
    "            for i in range(len(class_to_feat_chi_tup[nc]) // (num_class - 1)):\n",
    "                w = class_to_feat_chi_tup[nc][i][0]\n",
    "                class_to_feat_set[c].add(w) # if many overlapping words across class_to_feat_chi_tup, then fewer features\n",
    "\n",
    "class_to_feat_list_sort_by_chi = { c: sorted(list(class_to_feat_set[c])) for c in class_list }\n",
    "class_to_feat_to_index = { c: {} for c in class_list }\n",
    "for c in class_to_feat_list_sort_by_chi:\n",
    "    for i in range(len(class_to_feat_list_sort_by_chi[c])):\n",
    "        class_to_feat_to_index[c][class_to_feat_list_sort_by_chi[c][i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "pp.pprint(class_to_feat_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits: Dr. Jason Brownlee\n",
    "from random import seed, randrange\n",
    "seed(4248)\n",
    "\n",
    "# Split data_mat into num_folds number of folds\n",
    "def get_folds(data_mat, num_folds):\n",
    "    folds = []\n",
    "    data_clone = list(data_mat)\n",
    "    fold_size = int(len(data_mat) / num_folds)\n",
    "    for i in range(num_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(data_clone))\n",
    "            fold.append(data_clone.pop(index))\n",
    "        folds.append(fold)\n",
    "    return folds\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def get_accuracy(predicted, actual):\n",
    "    num_correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if predicted[i] == actual[i]:\n",
    "            num_correct += 1\n",
    "    return num_correct / len(actual) * 100\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def get_cross_validation_scores(data_mat, algorithm, num_folds, *args):\n",
    "    folds = get_folds(data_mat, num_folds)\n",
    "    scores = []\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = [row for fold in train_set for row in fold]\n",
    "        test_set = []\n",
    "        for row in fold:\n",
    "            row_clone = list(row)\n",
    "            test_set.append(row_clone)\n",
    "            row_clone[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = get_accuracy(predicted, actual)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = weights[0]\n",
    "    for i in range(len(row) - 1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1 if activation >= 0 else 0\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, alpha, max_iterations = 1000):\n",
    "    weights = [0 for i in range(len(train[0]))]\n",
    "    for _ in range(max_iterations):\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            weights[0] = weights[0] + alpha * error\n",
    "            for i in range(len(row) - 1):\n",
    "                weights[i + 1] += alpha * error * row[i]\n",
    "    return weights\n",
    "\n",
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, alpha, max_iterations):\n",
    "    predictions = list()\n",
    "    weights = train_weights(train, alpha, max_iterations)\n",
    "    print(weights)\n",
    "    for row in test:\n",
    "        prediction = predict(row, weights)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "class_to_feat_mat = { c: [] for c in class_list }\n",
    "for c in class_list:\n",
    "    for d in class_list:\n",
    "        texts = class_to_text[d]\n",
    "        num_texts = len(texts)\n",
    "        texts = iter(texts)\n",
    "        if c != d:\n",
    "            num_texts_to_train = int((1 - train_ratio) * num_texts)\n",
    "        else:\n",
    "            num_texts_to_train = num_texts\n",
    "        for i in range(num_texts_to_train):\n",
    "            text = next(texts)\n",
    "            feat_vec = [0 for i in range(len(class_to_feat_to_index[d]) + 1)]\n",
    "            for word in text_to_freq[text]:\n",
    "                if word in class_to_feat_to_index[d]:\n",
    "                    index = class_to_feat_to_index[d][word]\n",
    "                    feat_vec[index] = text_to_freq[text][word]\n",
    "            feat_vec[-1] = 1 if c == d else 0\n",
    "            class_to_feat_mat[c].append(feat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = class_to_feat_mat\n",
    "num_folds_list = [5] # [5, 10]\n",
    "alpha_list = [0.05] # [0.02, 0.03, 0.05, 0.07, 0.1]\n",
    "max_iterations_list = [500] # [500, 1000, 2000]\n",
    "\n",
    "for num_folds in num_folds_list:\n",
    "    for alpha in alpha_list:\n",
    "        for max_iterations in max_iterations_list:\n",
    "            print('{}-fold cross validation'.format(num_folds))\n",
    "            print('Learning rate: {}, maximum number of iterations: {}'.format(alpha, max_iterations))\n",
    "            for c in class_list:\n",
    "                scores = get_cross_validation_scores(data[c], perceptron, num_folds, alpha, max_iterations)\n",
    "                print('Class: {}'.format(c))\n",
    "                print('Cross validation scores: {}'.format(scores))\n",
    "                print('Mean accuracy: {:.2f}%'.format(sum(scores) / num_folds))\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "pp.pprint(class_to_feat_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
