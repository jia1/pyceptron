{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4248 Assignment 3\n",
    "\n",
    "### Perceptron text classifier in Python with feature selection\n",
    "\n",
    "##### https://github.com/jia1/pyceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from string import punctuation\n",
    "from porter import PorterStemmer\n",
    "\n",
    "p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = os.path.abspath('tc')\n",
    "dst_dir = os.path.abspath('tc_proc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "classes, nxxs = [], ['n00', 'n01', 'n10', 'n11']\n",
    "class_to_text, text_to_freq = {}, {}\n",
    "class_to_feat = {}\n",
    "nxx_dict = { n: {} for n in nxxs }\n",
    "chi_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Split each line by space into tokens\n",
    "# B. Strip all default white space characters from each token\n",
    "# C. Remove punctuation from each token\n",
    "# D. Return a list of tokens which are not stop words\n",
    "\n",
    "def strip_and_filter_line(ln):\n",
    "    tokens = map(lambda t: t.strip().strip(punctuation).lower(), ln.split(' '))\n",
    "    return list(filter(lambda t: t and len(t) > 2 and t.isalpha() and t not in stop_list, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isIn(a, b):\n",
    "    return 1 if a in b else 0\n",
    "\n",
    "def isNotIn(a, b):\n",
    "    return 1 if a not in b else 0\n",
    "\n",
    "def count_nxx(nxx, w, c):\n",
    "    global classes, class_to_text, text_to_freq\n",
    "    answer = 0\n",
    "    if nxx == 'n00':\n",
    "        for class_name in filter(lambda x: x != c, classes):\n",
    "            for text in class_to_text[class_name]:\n",
    "                if isNotIn(w, text_to_freq[text]):\n",
    "                    answer += 1\n",
    "    elif nxx == 'n01':\n",
    "        for text in class_to_text[c]:\n",
    "            if isNotIn(w, text_to_freq[text]):\n",
    "                answer += 1\n",
    "    elif nxx == 'n10':\n",
    "        for class_name in filter(lambda x: x != c, classes):\n",
    "            for text in class_to_text[class_name]:\n",
    "                if isIn(w, text_to_freq[text]):\n",
    "                    answer += 1\n",
    "    elif nxx == 'n11':\n",
    "        for text in class_to_text[c]:\n",
    "            if isIn(w, text_to_freq[text]):\n",
    "                answer += 1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n00 is the number of training texts that do not contain w and are not in class c.\n",
    "# n01 is the number of training texts that do not contain w and are in class c.\n",
    "# n10 is the number of training texts that contain w and are not in class c.\n",
    "# n11 is the number of training texts that contain w and are in class c.\n",
    "\n",
    "def chi_square(w, c):\n",
    "    global nxxs, nxx_dict\n",
    "    ns_dict = {}\n",
    "    for n in nxxs:\n",
    "        if w not in nxx_dict[n]:\n",
    "            nxx_dict[n][w] = {}\n",
    "        if c not in nxx_dict[n][w]:\n",
    "            nxx_dict[n][w][c] = count_nxx(n, w, c)\n",
    "        ns_dict[n] = nxx_dict[n][w][c]\n",
    "    n00, n01, n10, n11 = ns_dict['n00'], ns_dict['n01'], ns_dict['n10'], ns_dict['n11']\n",
    "    return ((n11+n10+n01+n00)*(n11*n00-n10*n01)**2)/((n11+n01)*(n11+n10)*(n10+n00)*(n01+n00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_chi_dict(c, w, chi_square_value):\n",
    "    global chi_dict\n",
    "    if w not in chi_dict[c]:\n",
    "        chi_dict[c][w] = chi_square_value\n",
    "    else:\n",
    "        chi_dict[c][w] = max(chi_dict[c][w], chi_square_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feats():\n",
    "    global classes, chi_dict, class_to_feat\n",
    "    max_feat_len = sys.maxsize\n",
    "    feat_queue_dict = { c: [] for c in classes }\n",
    "    for c in chi_dict:\n",
    "        feat_queue_dict[c] = sorted(chi_dict[c].items(), key = lambda x: x[1], reverse = True)\n",
    "        max_feat_len = min(max_feat_len, len(feat_queue_dict[c]))\n",
    "    max_feat_len //= 100\n",
    "    class_to_feat = { c: feat_queue_dict[c][:max_feat_len] for c in feat_queue_dict }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select():\n",
    "    global classes, class_to_text, text_to_freq, class_to_feat\n",
    "    for c in classes:\n",
    "        for text in class_to_text[c]:\n",
    "            for w in text_to_freq[text]:\n",
    "                put_chi_dict(c, w, chi_square(w, c))\n",
    "                gen_feats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor this\n",
    "def pla(xn, yn, maxIter = 1000, w = np.zeros(3)):\n",
    "    N = xn.shape[0]\n",
    "    summ_w = w\n",
    "    rows = len(w)\n",
    "    for _ in range(maxIter):\n",
    "        i = nr.randint(N)\n",
    "        if(yn[i] != g(xn[i,:])):\n",
    "            w[0] += yn[i]\n",
    "            w[1] += yn[i]*xn[i][0]\n",
    "            w[2] += yn[i]*xn[i][1]\n",
    "            for j in range(rows):\n",
    "                summ_w[j] += w[j]\n",
    "    summ_w = map(lambda weight: weight/maxIter+1, summ_w)\n",
    "    return summ_w\n",
    "    # return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# Load all stop words into a list\n",
    "\n",
    "with open('stopword-list', 'r') as s:\n",
    "    stop_list = list(map(lambda ln: ln.strip(), s.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for curr_dir, sub_dir, files in os.walk(src_dir):\n",
    "    if not files:\n",
    "        classes = sub_dir\n",
    "        class_to_text = { c: set() for c in classes }\n",
    "        class_to_feat = { c: set() for c in classes }\n",
    "        chi_dict = { c: {} for c in classes }\n",
    "        continue\n",
    "    curr_class = re.split('[(\\\\\\\\)(\\\\)(\\/)]', curr_dir)[-1] # curr_dir.split('\\\\')[-1]\n",
    "    for file in files:\n",
    "        flat_text = []\n",
    "        freq_dict = {}\n",
    "        with open(os.path.join(curr_dir, file), 'r') as f:\n",
    "            processed_lines = map(lambda ln: strip_and_filter_line(ln), f.readlines())\n",
    "            for line in processed_lines:\n",
    "                flat_text.extend(list(map(lambda word: p.stem(word, 0, len(word) - 1), line)))\n",
    "            for word in flat_text:\n",
    "                if word not in freq_dict:\n",
    "                    freq_dict[word] = 1\n",
    "                else:\n",
    "                    freq_dict[word] += 1\n",
    "            fin_freq_dict = { word: freq for word, freq in freq_dict.items() if freq >= k }\n",
    "            if not fin_freq_dict:\n",
    "                fin_freq_dict = freq_dict\n",
    "            sum_freq = sum(fin_freq_dict.values())\n",
    "            normalized_freq_dict = { word: freq / sum_freq for word, freq in fin_freq_dict.items() }\n",
    "            class_to_text[curr_class].add(file)\n",
    "            text_to_freq[file] = normalized_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "feature_select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classes)\n",
    "# print(nxxs)\n",
    "# print(class_to_text)\n",
    "# print(text_to_freq)\n",
    "# print(nxx_dict)\n",
    "# print(class_to_feat)\n",
    "# print(len(class_to_feat['c1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
